load("C:/Users/Herdis/Desktop/Master/data_for_lecture_12_2021.Rdata")
rm(list = ls())
load("df.Rdata")
setwd("~/GitHub/thesis")
rm(list = ls())
load("df.Rdata")
rm(list = ls())
load("df.Rdata")
# Stopwords
# Stopwords do not have capital letters or punctuation - remove
corpus =
df$text %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
# Libraries
library(rvest)
library(RSelenium)
library(httr)
library(stringr)
library(BatchGetSymbols)
library(lexicon)
library(translateR)
library(lubridate)
library(RCurl)
library(dplyr)
library(tm)
library(stopwords)
library(quanteda)
rm(list = ls())
load("df.Rdata")
# Stopwords
# Stopwords do not have capital letters or punctuation - remove
corpus =
df$text %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
# Change the articles to a corpus format
corpus =
corpus(corpus)
# Tokenize and remove stopwords
stopw = stopwords::stopwords(language = "no")
toks = corpus %>%
tokens() %>%
tokens_remove(stopw)
load("LMNorsk.RData")
sum(LM.norsk$y == 1)  # Number of positive words
sum(LM.norsk$y == -1) # Number of negative words
# Remove duplicates and more than one word translations
nrow((distinct(as.data.frame(LM.norsk$translatedContent))))
which(duplicated(LM.norsk$translatedContent))
LM.norsk <- LM.norsk[!duplicated(LM.norsk$translatedContent), ]
which(sapply(strsplit(LM.norsk$translatedContent, " "), length)>1)
LM.norsk <- LM.norsk[!sapply(strsplit(LM.norsk$translatedContent, " "), length)>1, ]
# Change format of columns so LM.norsk looks exactly like the untranslated dictionary
LM.norsk =
LM.norsk %>%
select("x" = translatedContent, y)
# SENTIMENT SCORE
# It is important that the sentiment dictionary and stopwords dictionary do not
# overlap, that would result in an unreliable sentiment score.
inner_join(data.frame(x = stopw),LM.norsk,by="x")
corpus = rep(NA,nrow(df))
for(i in 1:nrow(df)){
for(j in 1:length(corpus)){
corpus[j] =
df$text[i] %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
corpus =
corpus(corpus)
}
corpus[1]
corpus[[1]]
rm(list = ls())
load("df.Rdata")
# Stopwords
# Stopwords do not have capital letters or punctuation - remove
corpus =
df$text %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
# Change the articles to a corpus format
corpus =
corpus(corpus)
# Tokenize and remove stopwords
stopw = stopwords::stopwords(language = "no")
toks = corpus %>%
tokens() %>%
tokens_remove(stopw)
load("LMNorsk.RData")
sum(LM.norsk$y == 1)  # Number of positive words
sum(LM.norsk$y == -1) # Number of negative words
# Remove duplicates and more than one word translations
nrow((distinct(as.data.frame(LM.norsk$translatedContent))))
which(duplicated(LM.norsk$translatedContent))
LM.norsk <- LM.norsk[!duplicated(LM.norsk$translatedContent), ]
which(sapply(strsplit(LM.norsk$translatedContent, " "), length)>1)
LM.norsk <- LM.norsk[!sapply(strsplit(LM.norsk$translatedContent, " "), length)>1, ]
# Change format of columns so LM.norsk looks exactly like the untranslated dictionary
LM.norsk =
LM.norsk %>%
select("x" = translatedContent, y)
# SENTIMENT SCORE
# It is important that the sentiment dictionary and stopwords dictionary do not
# overlap, that would result in an unreliable sentiment score.
inner_join(data.frame(x = stopw),LM.norsk,by="x")
corpus = rep(NA,nrow(df))
for(i in 1:length(corpus)){
for(j in 1:nrow(df)){
corpus[j] =
df$text[i] %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
}
corpus =
corpus(corpus)
}
corpus[[1]]
rm(list = ls())
load("df.Rdata")
# Stopwords
# Stopwords do not have capital letters or punctuation - remove
corpus =
df$text %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
# Change the articles to a corpus format
corpus =
corpus(corpus)
# Tokenize and remove stopwords
stopw = stopwords::stopwords(language = "no")
toks = corpus %>%
tokens() %>%
tokens_remove(stopw)
load("LMNorsk.RData")
sum(LM.norsk$y == 1)  # Number of positive words
sum(LM.norsk$y == -1) # Number of negative words
# Remove duplicates and more than one word translations
nrow((distinct(as.data.frame(LM.norsk$translatedContent))))
which(duplicated(LM.norsk$translatedContent))
LM.norsk <- LM.norsk[!duplicated(LM.norsk$translatedContent), ]
which(sapply(strsplit(LM.norsk$translatedContent, " "), length)>1)
LM.norsk <- LM.norsk[!sapply(strsplit(LM.norsk$translatedContent, " "), length)>1, ]
# Change format of columns so LM.norsk looks exactly like the untranslated dictionary
LM.norsk =
LM.norsk %>%
select("x" = translatedContent, y)
# SENTIMENT SCORE
# It is important that the sentiment dictionary and stopwords dictionary do not
# overlap, that would result in an unreliable sentiment score.
inner_join(data.frame(x = stopw),LM.norsk,by="x")
# Testing
score = rep(0,nrow(df))
}
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(LM.norsk %>% filter(x %in% toks[[i]]))$y
}
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(LM.norsk %>% filter(x %in% toks[[t]]))$y
}
temp = LM.norsk %>% filter(x %in% toks[[1]])
View(temp)
temp = sum(LM.norsk %>% filter(x %in% toks[[1]]))$y
?filter
temp = sum(filter(LM.norsk, x %in% toks[[1]])$y)
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(filter(LM.norsk, x %in% toks[[t]]))$y
}
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(filter(LM.norsk, x %in% toks[[t]])$y)
}
sum(score)
# Testing
score = rep(0,nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
for(n in 1:nrow(LM.norsk)){
score[s] = sum(filter(LM.norsk, x[n] %in% toks[[t]])$y)
}
sum(score)
for(t in 1:length(toks)){
for(s in 1:length(score)){
for(lm in 1:nrow(LM.norsk)){
score[s] = sum(filter(LM.norsk, x[lm] %in% toks[[t]])$y)
}
# Testing
score = rep(0,nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
for(lm in 1:nrow(LM.norsk)){
score[s] = sum(filter(LM.norsk, x[lm] %in% toks[[t]])$y)
}
for(t in 1:length(toks)){
for(s in 1:length(score)){
for(lm in 1:nrow(LM.norsk)){
score[s] = sum(filter(LM.norsk, x %in% toks[[t]])$y)
}
temp = sum(filter(LM.norsk, x %in% toks[[1]])$y)
sum(filter(LM.norsk, x %in% toks[[1]])$y)
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(filter(LM.norsk, x %in% toks[[t]])$y)
}
# Testing
score = rep(0,nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(filter(LM.norsk, x %in% toks[[t]])$y)
}
# Testing
score = rep(0,nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(filter(LM.norsk, x %in% toks[[t]])$y)/length(toks[[t]])
}
# Testing
df$score = 0
length(toks)
for(t in 1:length(toks)){
for(s in 1:nrow(df)){
df$score[s] = sum(filter(LM.norsk, x %in% toks[[t]])$y)/length(toks[[t]])
}
sum(filter(LM.norsk,x%in%toks[[1]])$y)/length(toks[[1]])
sum(filter(LM.norsk$x%in%toks[[1]])$y)/length(toks[[1]])
sum(filter(LM.norsk,x%in%toks[[2]])$y)/length(toks[[2]])
length(toks[[1]])
length(toks[1])
length(toks)
# Testing
df$score = 0
for(t in 1:length(toks)){
for(s in 1:nrow(df)){
df$score[s] = sum(filter(LM.norsk, x %in% toks[t])$y)/length(toks[t])
}
# Testing
score = rep(0,nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(filter(LM.norsk, x %in% toks[t])$y)/length(toks[t])
}
sum(score)
# Testing
score = rep(0,nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(filter(LM.norsk, x %in% toks[[t]])$y)/length(toks[[t]])
}
length(score)
length(score[1])
# Testing
score = rep(0,nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
score[s] = sum(filter(LM.norsk, x %in% toks[t])$y)/length(toks[t])
}
sum(filter(LM.norsk, x %in% toks[[1]])$y)
sum(filter(LM.norsk, x %in% toks[[1]])$y)/length(toks[[1]])
class(sum(filter(LM.norsk, x %in% toks[[1]])$y)/length(toks[[1]]))
sum(filter(LM.norsk, x %in% toks[[1]])$y)/length(toks[[1]])
filter(LM.norsk,x %in% toks[[1]])
sum(filter(LM.norsk,x %in% toks[[1]])$y)
# Testing
score = rep(0,nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
temp = sum(filter(LM.norsk, x %in% toks[[t]])$y) / length(toks[[t]])
score[s] = temp
}
# Testing
df$score = 0
for(t in 1:length(toks)){
for(s in 1:nrow(df)){
temp = sum(filter(LM.norsk, x %in% toks[[t]])$y) / length(toks[[t]])
df$score[s] = temp
}
View(df)
# Testing
score = vector()
# Testing
score = vector(length = nrow(df))
for(t in 1:length(toks)){
for(s in 1:length(score)){
temp = sum(filter(LM.norsk, x %in% toks[[t]])$y) / length(toks[[t]])
score[s] = temp
}
# Testing
score = vector()
# Testing
score = vector(1,2,3)
# Testing
score = vector(c(1,2))
?vector
# Testing
score = vector(c(1))
# Testing
score = vector(1)
# Testing
score = vector()
# Testing
score = vector()
vector = append(score,
sum(filter(LM.norsk, x %in% toks[[1]])$y) / length(toks[[1]]))
score = append(score,
sum(filter(LM.norsk, x %in% toks[[1]])$y) / length(toks[[1]]))
score = append(score,
sum(filter(LM.norsk, x %in% toks[[2]])$y) / length(toks[[2]]))
# Testing
score = vector()
for(t in 1:length(toks)){
score = append(score,
sum(filter(LM.norsk, x %in% toks[[t]])$y) / length(toks[[i]]))
}
for(t in 1:length(toks)){
score = append(score,
sum(filter(LM.norsk, x %in% toks[[t]])$y) / length(toks[[t]]))
}
# Test
sum(filter(LM.norsk, x %in% toks[[1]])$y) / length(toks[[1]]) == score[1]
sum(filter(LM.norsk, x %in% toks[[3184]])$y) / length(toks[[3184]]) == score[3184]
# Insert into df
df$score = score
View(df)
# Basic linear regression of score~av.price
lin = lm(score~av.price)
# Basic linear regression of score~av.price
lin = lm(score~av.price,data = df)
lin
pred = predict(lin)
# Basic linear regression of score~av.price
lin = lm(av.price~score,data = df)
lin
# Insert into df
df$sentiment = score
set.seed(123)
ind <- df$date <= 2018-12-31
train <- df[ind,]
test <- df[-ind,]
# with full data
simplelog <- glm(av.price~sentiment, data = df, family = binomial())
# with full data
simplelog <- glm(av.price~sentiment, data = df, family = binomial
# with full data
simplelog <- glm(av.price~sentiment, data = df, family = binomial)
# with full data
simplelog <- glm(av.price~sentiment, data = df, family = binomial)
max(df$sentiment)
min(df$sentiment)
max(df$sentiment)+min(df$sentiment())
max(df$sentiment)+min(df$sentiment)
# Lasso/ridge regression:
xtrain <- model.matrix(av.price~sentiment, train)
ytrain <- train$av.price
xtest <- model.matrix(av.price~sentiment,test)
ytest <- test$av.price
lambdamin <- cv.glmnet(xtrain,ytrain, alpha = 1, nfold = n)$lambda.min
library(glmnet)
lambdamin <- cv.glmnet(xtrain,ytrain, alpha = 1, nfold = n)$lambda.min
# Libraries
library(rvest)
library(RSelenium)
library(httr)
library(stringr)
library(BatchGetSymbols)
library(lexicon)
library(translateR)
library(lubridate)
library(RCurl)
library(dplyr)
library(tm)
library(stopwords)
library(quanteda)
library(boot)
library(e1071)
library(ROCR)
rm(list = ls())
load("df.Rdata")
# Stopwords
# Stopwords do not have capital letters or punctuation - remove
corpus =
df$text %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
# Change the articles to a corpus format
corpus =
corpus(corpus)
# Tokenize and remove stopwords
stopw = stopwords::stopwords(language = "no")
toks = corpus %>%
tokens() %>%
tokens_remove(stopw)
load("LMNorsk.RData")
sum(LM.norsk$y == 1)  # Number of positive words
sum(LM.norsk$y == -1) # Number of negative words
# Remove duplicates and more than one word translations
nrow((distinct(as.data.frame(LM.norsk$translatedContent))))
which(duplicated(LM.norsk$translatedContent))
LM.norsk <- LM.norsk[!duplicated(LM.norsk$translatedContent), ]
which(sapply(strsplit(LM.norsk$translatedContent, " "), length)>1)
LM.norsk <- LM.norsk[!sapply(strsplit(LM.norsk$translatedContent, " "), length)>1, ]
# Change format of columns so LM.norsk looks exactly like the untranslated dictionary
LM.norsk =
LM.norsk %>%
select("x" = translatedContent, y)
# SENTIMENT SCORE
# It is important that the sentiment dictionary and stopwords dictionary do not
# overlap, that would result in an unreliable sentiment score.
inner_join(data.frame(x = stopw),LM.norsk,by="x")
# Create actual sentiment score (sum of +1 and -1 values of positive/negative words
# divided by the total number of words)
score = vector()
for(t in 1:length(toks)){
score = append(score,
sum(filter(LM.norsk, x %in% toks[[t]])$y) / length(toks[[t]]))
}
# Test
sum(filter(LM.norsk, x %in% toks[[1]])$y) / length(toks[[1]]) == score[1]
sum(filter(LM.norsk, x %in% toks[[3184]])$y) / length(toks[[3184]]) == score[3184]
# Insert into df
df$sentiment = score
df <- df[!df$dir=="no change",]
# change y value to class factor
df$dir <- as.factor(df$dir)
set.seed(123)
n = nrow(df)
n.train = floor(0.8*n)
n.test = n.train+1
train = df[1:n.train,]
test = df[n.test:n,]
xtrain <- model.matrix(av.price~sentiment, train)
ytrain <- train$av.price
xtest <- model.matrix(av.price~sentiment,test)
ytest <- test$av.price
lambdamin <- cv.glmnet(xtrain,ytrain, alpha = 1, nfold = n)$lambda.min
lasso <- glmnet(xtrain, ytrain, alpha = 1, lambda = lambdamin)
lassopred <- predict(lasso, xtest)
lassomse <- mean((ytest-lassopred)^2)
conf.mat5 <- table(test$dir, lassopred)
conf.mat5
accuracy5 <- sum(diag(conf.mat5))/sum(conf.mat5)
accuracy5
val.set.err5 <- (conf.mat5[1,2]+conf.mat5[2,1])/(n/2)
val.set.err5
predsvmreg <- predict(svmreg, test, type = "response")
conf.mat5 <- table(test$dir, lassopred, type = "response")
conf.mat5 <- table(test$dir, lassopred, type = "response") > 0.5
conf.mat5
lambdamin <- cv.glmnet(xtrain,ytrain, alpha = 0, nfold = n)$lambda.min
lasso <- glmnet(xtrain, ytrain, alpha = 0, lambda = lambdamin)
lassopred <- predict(lasso, xtest)
conf.mat5 <- table(test$dir, lassopred)
conf.mat5
accuracy5 <- sum(diag(conf.mat5))/sum(conf.mat5)
accuracy5
conf.mat5 <- table(test$dir, lassopred, type = "response")
conf.mat5 <- table(test$dir, lassopred) > 0.5
conf.mat5
accuracy5 <- sum(diag(conf.mat5))/sum(conf.mat5)
accuracy5
val.set.err5 <- (conf.mat5[1,2]+conf.mat5[2,1])/(n/2)
val.set.err5
lambdamin <- cv.glmnet(xtrain,ytrain, alpha = 1, nfold = n)$lambda.min
lasso <- glmnet(xtrain, ytrain, alpha = 1, lambda = lambdamin)
lassopred <- predict(lasso, xtest)
conf.mat5 <- table(test$dir, lassopred)
conf.mat5
accuracy5 <- sum(diag(conf.mat5))/sum(conf.mat5)
accuracy5
val.set.err5 <- (conf.mat5[1,2]+conf.mat5[2,1])/(n/2)
val.set.err5
summary(lasso)
