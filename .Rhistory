html <- read_html("2019-Q3.html")  # HTML code from DN
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- html %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
# Are there duplicate urls?
nrow(url.list)
nrow(unique(urls))
nrow(unique(url.list))
# Extract URLs and dates for each article
html <- read_html("2019-Q2.html")  # HTML code from DN
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- html %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
# Are there duplicate urls?
nrow(url.list)
nrow(unique(url.list))
# Extract text from each article
text <- list()
for (url in URLs) {
jump <- session %>%
session_jump_to(url)  # Jump to each URL logged in
html <- read_html(jump) %>%
html_nodes("article") %>%
html_nodes("section") %>%
html_nodes("p")
text <- rbind(text, toString(html))
}
# Log in to DN subscription
# Only run after having closed R/cleaned environment!
url <- "https://www.dn.no/auth/login"
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36"
session <- session(url, user_agent(uastring))
form <- html_form(session)[[1]]
fill <- html_form_set(form,
username = "herdisbirta1@gmail.com",
password = "Herdiserbest")
session_submit(session, fill, submit = NULL, config(referer = session$url))
# Extract text from each article
text <- list()
for (url in URLs) {
jump <- session %>%
session_jump_to(url)  # Jump to each URL logged in
html <- read_html(jump) %>%
html_nodes("article") %>%
html_nodes("section") %>%
html_nodes("p")
text <- rbind(text, toString(html))
}
# Extract URLs and dates for each article
html <- read_html("2019-Q1.html")  # HTML code from DN
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- html %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
# Are there duplicate urls?
nrow(url.list)
nrow(unique(url.list))
# Are there duplicate urls?
nrow(url.list)==nrow(unique(url.list))
# Extract URLs and dates for each article
html <- read_html("2019-Q2.html")  # HTML code from DN
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- html %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
# Are there duplicate urls?
nrow(url.list)==nrow(unique(url.list))
# Extract URLs and dates for each article
html <- read_html("2016-Q4.html")  # HTML code from DN
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- html %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
# Are there duplicate urls?
nrow(url.list)==nrow(unique(url.list))
# Extract URLs and dates for each article
html <- read_html("2016-Q3.html")  # HTML code from DN
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- html %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
# Are there duplicate urls?
nrow(url.list)==nrow(unique(url.list))
# Extract URLs and dates for each article
html <- read_html("2016-Q2.html")  # HTML code from DN
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- html %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
# Are there duplicate urls?
nrow(url.list)==nrow(unique(url.list))
# Extract URLs and dates for each article
html <- read_html("2016-Q1.html")  # HTML code from DN
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- html %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
# Are there duplicate urls?
nrow(url.list)==nrow(unique(url.list))
for(year in years){
for(quarter in quarters){
html = read_html(paste0(year,"-Q",quarter,".html"))
}
years = 2014:2019
quarters = 1:4
for(year in years){
for(quarter in quarters){
html = read_html(paste0(year,"-Q",quarter,".html"))
}
View(html)
html
for(year in years){
for(quarter in quarters){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
}
View(`2014-Q1`)
'2014-Q1'
`2014-Q1`
html
wd
wd()
dir()
for(year in years){
for(quarter in quarters){
htmltools::save_html(tagList(
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quart))),
file = "/DN.html"
))
}
library(htmltools)
for(year in years){
for(quarter in quarters){
save_html(tagList(
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quart))),
file = "/DN.html"
))
}
for(year in years){
for(quarter in quarters){
save_html(tagList(
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quarter))),
file = "/DN.html"
))
}
for(year in years){
for(quarter in quarters){
save_html(tagList(
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quarter,".html"))),
file = "/DN.html"
))
}
for(year in years){
for(quarter in quarters){
save_html(tagList(
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quarter,".html"))),
file = "/files/DN.html"
))
}
for(year in years){
for(quarter in quarters){
save_html(tagList(
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quarter,".html"))),
file = "C:/Users/Herdis/Desktop/thesis/DN.html"
))
}
for(year in years){
for(quarter in quarters){
save_html(tagList(
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quarter,".html"))),
file = "C://Users//Herdis//Desktop//thesis//DN.html"
))
}
for(year in years){
for(quarter in quarters){
save_html(tagList(
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html"))),
file = "C://Users//Herdis//Desktop//thesis//DN.html"
))
}
for(year in years){
for(quarter in quarters){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
htmls = save_html(tagList(paste0(year,"-Q",quarter,".html")))
}
for(year in years){
for(quarter in quarters){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
save_html(tagList(paste0(year,"-Q",quarter,".html")),
file = "C://Users//Herdis//Desktop//thesis//DN.html")
}
for(year in years){
for(quarter in quarters){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
save_html(tagList(paste0(year,"-Q",quarter,".html")),
file = "C://Users//Herdis//Desktop//thesis//DNfiles.html")
}
for(year in years){
for(quarter in quarters){
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
}
for(year in years){
for(quarter in quarters){
assign(paste0(year,"Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
htmlnames = paste0(year,"Q",quarter)
}
for(year in years){
for(quarter in quarters){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
htmlnames[year] = paste0(year,"-Q",quarter)
}
htmlnames = rep(NA,6*4)
6*4
htmlnames = rep(NA,24)
htmlnames = 1:24
for(year in years){
for(quarter in quarters){
for(name in htmlnames){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
htmlnames[name] = paste0(year,"-Q",quarter)
}
html <- list.files(pattern="\\.(html)$") # get just .htm and .html files
files = list.files(pattern="\\.(html)$") # get just .html files
temp = c()
x <- list()
files = list.files(pattern="\\.(html)$") # get just .html files
for (i in nrow(files)) {
x[[i]] <- read_html(paste0(files[i]))
}
rm(list = ls())
files = list.files(pattern="\\.(html)$") # get just .html files
lapply(filestoread, function(x) try(read_html(x)))
lapply(files, function(x) try(read_html(x)))
articles <- 1
URLs <- list()
Dates <- list()
for (article in articles) {
URLs <- htmls %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
htmls = lapply(files, function(x) try(read_html(x)))
for (article in articles) {
URLs <- htmls %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
for (article in articles) {
URLs <- htmls %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
View(htmls)
for(year in years){
for(quarter in quarters){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
for (article in articles) {
URLs <- htmls %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
for(year in years){
for(quarter in quarters){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
}
# NEWS ARTICLE RETRIEVAL
years = 2014:2019
quarters = 1:4
for(year in years){
for(quarter in quarters){
assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))
for (article in articles) {
URLs <- htmls %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
for(i in 1:length(files)){
read_html(files)
}
files = list.files(pattern="\\.(html)$") # get list of .html files
for(i in 1:length(files)){
read_html(files)
}
for(i in 1:length(files)){
assign(files[i],read_html(files))
}
for(i in 1:length(files)){
assign(files[i],read_html(files))
}
length(files)
for(i in 1:length(files)){
assign(paste0(files[i]),read_html(files[i]))
}
for(i in 1:length(files)){
assign(paste0(gsub(".html","",files[i])),read_html(files[i]))
}
# NEWS ARTICLE RETRIEVAL
years = 2014:2019
quarters = 1:4
articles <- 1
URLs <- list()
Dates <- list()
files = list.files(pattern="\\.(html)$") # get list of .html files
for(year in years){
for(quarter in quarters){
URLs =
paste0(year,"-Q",quarter) %>%
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- html %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- data.frame(Dates, URLs)
}
?html_nodes
for(article in articles){
for(year in years){
for(quarter in quarters){
assign(paste0("URL-",year,"-Q",quarter),paste0(year,"-Q"))
}
for(article in articles){
for(year in years){
for(quarter in quarters){
assign(paste0("URL-",year,"-Q",quarter),paste0(year,"-Q",quarter) %>%
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href") )
}
View(`2015-Q2.html`)
`2014-Q1`
for(year in years){for(quarter in quarters){assign(paste0(year,"-Q",quarter),read_html(paste0(year,"-Q",quarter,".html")))}
)
rm(list = ls())
# NEWS ARTICLE RETRIEVAL
years = 2014:2019
quarters = 1:4
articles <- 1
URLs <- list()
Dates <- list()
files = list.files(pattern="\\.(html)$") # get list of .html files
for(i in 1:length(files)){
read_html(files[i])
}
for(i in 1:length(files)){
assign(paste0(gsub(".html","",files[i])),read_html(files[i]))
}
htmls = c(`2014-Q1`,`2014-Q2`)
View(htmls)
URLs = c()
URLs <- list()
Dates <- list()
url.list = list()
for(i in 1:length(htmls)){
URLs[i] = htmls %>%
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates[i] = htmls %>%
html_nodes("time") %>%
html_attr("datetime")
url.list = data.frame(Dates,URLs)
}
for(article in articles){
URLs = `2014-Q1` %>%
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates = htmls %>%
html_nodes("time") %>%
html_attr("datetime")
url.list = data.frame(Dates,URLs)
}
read_html("2014-Q1.html")
"2014-Q1a" = read_html("2014-Q1.html")
one2014 = read_html("2014-Q1.html")
one2014==`2014-Q1`
for(i in 1:length(files)){
assign(paste0("Q",quarter,".",year),read_html(files[i]))
}
for(i in 1:length(files)){
assign(paste0("Q",quarter,".",year),read_html(files[i]))
}
for(i in 1:length(files)){
assign(paste0("Q",quarter,".",year),read_html(files[i]))
}
files = list.files(pattern="\\.(html)$") # get list of .html files
for(i in 1:length(files)){
assign(paste0("Q",quarter,".",year),read_html(files[i]))
}
for(i in 1:length(files)){
for(year in years){
for(quarter in quarters){
assign(paste0("Q",quarter,".",year),read_html(files[i]))
}
for(year in years){
for(quarter in quarters){
assign(paste0("Q",quarter,".",year),read_html(year,"-Q",quarter,".html"))
}
rm(list = ls())
# NEWS ARTICLE RETRIEVAL
years = 2014:2019
quarters = 1:4
articles <- 1
URLs <- list()
Dates <- list()
url.list = list()
files = list.files(pattern="\\.(html)$") # get list of .html files
for(i in 1:length(files)){
for(year in years){
for(quarter in quarters){
assign(paste0("Q",quarter,".",year),read_html(files[i]))
}
rm(list = ls())
