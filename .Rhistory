# Libraries
library(readxl)
library(rvest)
library(RSelenium)
library(httr)
library(stringr)
library(BatchGetSymbols)
library(lexicon)
library(translateR)
library(lubridate)
library(RCurl)
library(dplyr)
library(tm)
library(stopwords)
library(quanteda)
library(boot)
library(e1071)
library(ROCR)
library(caret)
library(class)
library(gam)
library(tree)
library(randomForest)
library(kernlab)
setwd("~/GitHub/thesis")
rm(list = ls())
load("df1.Rdata")
# Stopwords
# Stopwords do not have capital letters or punctuation - remove
corpus =
df$text %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
# Change the articles to a corpus format
corpus =
corpus(corpus)
# Tokenize and remove stopwords
stopw = stopwords::stopwords(language = "no")
toks = corpus %>%
tokens() %>%
tokens_remove(stopw)
# works:
df$text[1] = as.String(toks[[1]])
View(df)
rm(list = ls())
load("df1.Rdata")
# Stopwords
# Stopwords do not have capital letters or punctuation - remove
corpus =
df$text %>%
tolower(.) %>%
gsub("[[:punct:]]","",.)
# Change the articles to a corpus format
corpus =
corpus(corpus)
# Tokenize and remove stopwords
stopw = stopwords::stopwords(language = "no")
toks = corpus %>%
tokens() %>%
tokens_remove(stopw)
toks[[1]]
View(df)
# Replace df$text with toks
for(i in 1:length(toks)){
for(j in 1:nrow(df)){
df$text[j] = as.String(toks[[i]])
}
