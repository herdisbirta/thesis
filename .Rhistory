load("C:/Users/Herdis/Desktop/Master/data_for_lecture_12_2021.Rdata")
rm(list = ls())
load("df.Rdata")
setwd("~/GitHub/thesis")
# Libraries
library(readxl)
library(rvest)
library(RSelenium)
library(httr)
library(stringr)
library(BatchGetSymbols)
library(lexicon)
library(translateR)
library(lubridate)
library(RCurl)
library(dplyr)
library(tm)
library(stopwords)
library(quanteda)
library(boot)
library(e1071)
library(ROCR)
library(caret)
library(class)
library(gam)
library(tree)
library(randomForest)
library(kernlab)
library(pROC)
# STOCK PRICE RETRIEVAL
# List of all registered companies
# Get list of all .xlsx files
file.list = list.files(pattern='*.xlsx')
# Read all files
y2014 = read_excel(path = file.list[1], skip = 9)   # Read 2014 file
y2014 = rbind(y2014[1:25,],    # OBX
y2014[29:150,],  # OB Match
y2014[155:185,]) # OB Standard and New
y2015 = read_excel(path = file.list[2], skip = 9)   # Read 2015 file
y2015 = rbind(y2015[1:25,],    # OBX
y2015[29:148,],  # OB Match
y2015[153:181,]) # OB Standard and New
y2016 = read_excel(path = file.list[3], skip = 9)   # Read 2016 file
y2016 = rbind(y2016[1:25,],    # OBX
y2016[29:151,],  # OB Match
y2016[155:180,]) # OB Standard and New
y2017 = read_excel(path = file.list[4], skip = 9)   # Read 2017 file
y2017 = rbind(y2017[1:25,],    # OBX
y2017[28:163,],  # OB Match
y2017[167:187,]) # OB Standard and New
y2018 = read_excel(path = file.list[5], skip = 9)   # Read 2018 file
y2018 = rbind(y2018[1:25,],    # OBX
y2018[28:158,],  # OB Match
y2018[162:186,]) # OB Standard and New
y2019 = read_excel(path = file.list[6], skip = 9)   # Read 2019 file
y2019 = rbind(y2019[1:25,],    # OBX
y2019[28:162,],  # OB Match
y2019[166:188,]) # OB Standard and New
# Combine
all.firms = rbind(y2014,y2015,y2016,y2017,y2018,y2019)
# Select relevant columns
all.firms =
all.firms %>%
select("Company" = OBX, ticker = ...20)
# Find unique company names
all.firms = all.firms[!duplicated(all.firms$ticker),]
# Remove 'y20XX' data frames
rm(list = ls(pattern = "^y20"))
# Manually rename wrong tickers
all.firms$ticker =
all.firms$ticker %>%
gsub("AKERBP","AKRBP",.) %>%
gsub("AKA","AKAST",.) %>%
gsub("ARCHER","ARCH",.) %>%
gsub("ASETEK","ASTK",.) %>%
gsub("AVANCE","AGAS",.) %>%
gsub("BEL","BELCO",.) %>%
gsub("BON","BONHR",.) %>%
gsub("BDRILL","BORR",.) %>%
gsub("BOUVET","BOUV",.) %>%
gsub("COV","CONTX",.) %>%
gsub("CRAYON","CRAYN",.) %>%
gsub("FKRAFT","FKRFT",.) %>%
gsub("NANO","NANOV",.) %>%
gsub("PROTCT","PROT",.) %>%
gsub("REC","RECSI",.) %>%
gsub("SALMON","SACAM",.) %>%
gsub("SBX","GEG",.) %>%
gsub("WWASA","WWI",.) %>%
gsub("STRONG","STRO",.) %>%
gsub("VISTIN","VISTN",.) %>%
gsub("ASC","ABG",.) %>%
gsub("AXA","ACR",.) %>%
gsub("NORBIT","NORBT",.) %>%
gsub("OTELLO","OTEC",.) %>%
gsub("SAS NOK","SASNO",.) %>%
gsub("SSO","SCATC",.) %>%
gsub("ULTIMO","ULTI",.) %>%
gsub("WALWIL","WAWI",.) %>%
gsub("SCH","SCHA",.) %>%
gsub("WBULK","WEST",.)
# Edit ticker to be on the format "TICKER.OL"
all.firms$ticker = paste0(all.firms$ticker, ".OL")
# Create vector of tickers
all.tickers = as.vector(all.firms$ticker)
# Manually change company names to their more "referred-to" versions
# (Yara international becomes Yara, etc.), remove "," and "."
all.firms$Company =
all.firms$Company %>%
gsub("Norwegian Air Shuttle", "Norwegian Air",.) %>%
gsub("Yara International","Yara",.) %>%
gsub("AKVA Group", "AKVA",.) %>%
gsub("ABG Sundal Collier Holding","ABG Sundal Collier",.) %>%
gsub("Avance Gas Holding","Avance Gas",.) %>%
gsub("BW Offshore Limited","BW Offshore",.) %>%
gsub("Electromagnetic Geoservices", "EMGS",.) %>%
gsub("InterOil Exploration and Production","Interoil",.) %>%
gsub("Lerøy Seafood Group","Lerøy Seafood",.) %>%
gsub("Questerre Energy Corporation", "Questerre",.) %>%
gsub("SAS AB", "SAS", .) %>%
gsub("Subsea 7","Subsea",.) %>%
gsub("Petroleum Geo-Services", "Petroleum Geo Services",.) %>%
gsub("Q-Free", "Q Free",.) %>%
gsub("Tomra Systems", "Tomra",.) %>%
gsub("Voss Veksel- og Landmandsbank","Voss Veksel og Landmandsbank",.) %>%
gsub("Link Mobility Group","Link Mobility",.) %>%
gsub("Crayon Group Holding", "Crayon",.) %>%
gsub("Insr Insurance Group", "Insr Insurance",.) %>%
gsub("NEXT Biometrics Group", "Next Biometrics",.) %>%
gsub("Questerre Energy Corporation","Questerre",.) %>%
gsub("Jinhui Shipping and Transportation","Jinhui Shipping",.) %>%
gsub("TGS-NOPEC Geophysical Company","TGS Nopec",.) %>%
gsub("Golden Ocean Group","Golden Ocean",.) %>%
gsub("Otello Corporation","Otello",.) %>%
gsub("Fjordkraft Holding", "Fjordkraft", .) %>%
gsub("PCI Biotech Holding","PCI Biotech",.) %>%
gsub("S.D. Standard Drilling", "SD Standard Drilling",.) %>%
gsub("TietoEVRY","Tieto",.) %>%
gsub("Oceanteam Shipping","Oceanteam",.) %>%
gsub("Gaming Innovation Group", "Gaming Innovation",.) %>%
gsub("Panoro Energy","Panoro",.) %>%
gsub("Havyard Group","Havyard",.) %>%
gsub("American Shipping Company","American Shipping",.) %>%
gsub("Vistin Pharma", "Vistin",.) %>%
gsub("Gjensidige Forsikring","Gjensidige",.) %>%
gsub("NTS","NTS Group",.) %>%
gsub("\\.","",.) %>%
gsub("\\,","",.)
# Remove company names that sound too similar (searching for "Aker" will give
# results of "Aker" and "Aker BP" for example, "Wilh Wilhelmsen Holding"
# has both "ser A" and "ser B")
all.firms =
all.firms %>%
subset(Company != "Aker" &
Company != "Hafslund ser A" & Company != "Hafslund ser B" &
Company != "Reach Subsea" & Company != "SpareBank 1" &
Company != "Wilh Wilhelmsen Holding ser A" &
Company != "Wilh Wilhelmsen Holding ser B" &
Company != "Schibsted ser A" &
Company != "Schibsted ser B" &
Company != "Odfjell ser A" &
Company != "Odfjell ser B" &
Company != "B2Holding" &
Company != "Solstad Offshore ser B" &
Company != "Adevinta ser A")
# Using tickers vector to obtain stock data from Yahoo Finance
all.stocks <- BatchGetSymbols(tickers = all.tickers,
first.date = "2014-01-01",
last.date = "2019-12-31",
freq.data = "daily",
do.cache = FALSE,
thresh.bad.data = 0)
# How many companies do we have? (163)
# We originally had 258 companies and tickers, some tickers didn't have
# any info (deregistered or acquired by other companies and therefore no info),
# some companies had to be removed to eliminate possible confusion when searching
sum(ifelse(all.stocks$df.control$threshold.decision=="KEEP",1,0))
# Convert stock information into a data frame
stocks = all.stocks$df.tickers
# Finalizing the company list
# Add company name (for searching purposes)
stocks = left_join(stocks,all.firms,by="ticker")
stocks$Company = tolower(stocks$Company)
# Get the names of the companies we have stock price data for
companies = unique(stocks$Company)
# Add company names more used/or changed during time period
companies = c(companies,c("statoil", "marine harvest",
"af group","solstad farstad",
"vekselbanken","bergen group",
"vardia insurance",
"skandiabanken","psi",
"opera software",
"apptix","noreco",
"tts group","pgs",
"namsos traffikkselskap"))
# Calculate a daily price measure
for(i in 1:nrow(stocks)){
stocks$av.price[i] = (stocks$price.open[i]+stocks$price.close[i])/2
}
# Only select columns we are interested in
stocks =
stocks %>%
select(Company,ticker,"date"=ref.date,price.close,price.open,av.price)
# Create direction-column
stocks$diff = stocks$av.price - lag(stocks$av.price)
stocks$dir = ifelse(stocks$diff == 0, "no change", ifelse(stocks$diff > 0, "up", "down"))
# Make sure that the last price observation of Akastor and the first observation
# of Aker BP (for example) are not calculated together, first observation of each
# company gives NA in the dir-column
# Note: gives error, still works
for(i in 2:nrow(stocks)){
if(stocks$Company[i] != stocks$Company[i-1])
stocks$dir[i] = NA
}
save(stocks,companies,file="stocks.Rdata")
# NEWS ARTICLE RETRIEVAL
# Extract URLs and dates for each article
articles <- 1
url.list <- as.character()
date.list <- as.character()
files = sort(list.files(pattern="\\.(html)$"), decreasing = T) # get list of .html files
for (file in files) {
URLs <- read_html(file) %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- read_html(file) %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- append(url.list, URLs)
date.list <- append(date.list, Dates)
}
# Are there duplicated URLs?
nrow((distinct(as.data.frame(url.list))))
which(duplicated(url.list) == TRUE)
# Removing wrong/not working URLs
grep("notis", url.list)
grep("https://www.dn.no/marked/2-1-", url.list)
grep("https://www.dn.no/arbeidsliv/2-1", url.list)
grep("https://www.dn.no/personvern/handel/slar-alarm-om-personvern/1-1-5397744", url.list)
grep("https://www.dn.no/borsbarna/finans/mener-aksjer-larer-ungene-om-frykt-og-gradighet/1-1-5331779", url.list)
grep("https://www.dn.no/dagligvare/handel/coop-vil-selge-103-butikker/1-1-5308961", url.list)
url.list <- url.list[-c(500, 1015, 1016, 4709, 4728, 4819, 5290, 5551, 5655, 5812, 6029, 6042, 6077, 7125, 10823, 15118, 15802, 16111)]
date.list <- date.list[-c(500, 1015, 1016, 4709, 4728, 4819, 5290, 5551, 5655, 5812, 6029, 6042, 6077, 7125, 10823, 15118, 15802, 16111)]
# Log in to DN subscription
# Only run after having closed R/cleaned environment!
url <- "https://www.dn.no/auth/login"
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36"
session <- session(url, user_agent(uastring))
form <- html_form(session)[[1]]
fill <- html_form_set(form,
username = "livewt@live.no",
password = "masterthesis123")
session_submit(session, fill, submit = NULL, config(referer = session$url))
# Extract text from each article
drafttext <- list()
