load("C:/Users/Herdis/Desktop/Master/data_for_lecture_12_2021.Rdata")
load("text.Rdata")
load("stocks.Rdata")
# Get the names of the companies we have stock price data for
companies = unique(stocks$Company)
# Test run - article 5 mentions at least one company (DNB)
test = df[5:20,1:3]
# Create a nice data frame for the results
mycols = c("text","date","url",companies)
df = data.frame(matrix(ncol = length(mycols),nrow = nrow(text)))
colnames(df) = mycols
df$date = text$date
df$text = text$text
df$url = text$url
# Test run - article 5 mentions at least one company (DNB)
test = df[5:20,1:3]
grep("DNB",tst)
grep("DNB",test)
# Test run - article 5 mentions at least one company (DNB)
test = df[5,1:3]
grep("DNB",test)
View(test)
load("text.Rdata")
load("stocks.Rdata")
# Get the names of the companies we have stock price data for
companies = unique(stocks$Company)
# Create a nice data frame for the results
mycols = c("text","date","url",companies)
df = data.frame(matrix(ncol = length(mycols),nrow = nrow(text)))
colnames(df) = mycols
df$date = text$date
df$text = text$text
df$url = text$url
View(df)
# NEWS ARTICLE RETRIEVAL
# Extract URLs and dates for each article
articles <- 1
url.list <- as.character()
date.list <- as.character()
files = sort(list.files(pattern="\\.(html)$"), decreasing = T) # get list of .html files
for (file in files) {
URLs <- read_html(file) %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- read_html(file) %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- append(url.list, URLs)
date.list <- append(date.list, Dates)
}
# Are there duplicate urls?
nrow((distinct(as.data.frame(url.list))))
which(duplicated(url.list) == TRUE)
# Removing wrong/not working URLs)
grep("notis", url.list)
grep("https://www.dn.no/marked/2-1-", url.list)
grep("https://www.dn.no/arbeidsliv/2-1", url.list)
grep("https://www.dn.no/personvern/handel/slar-alarm-om-personvern/1-1-5397744", url.list)
grep("https://www.dn.no/borsbarna/finans/mener-aksjer-larer-ungene-om-frykt-og-gradighet/1-1-5331779", url.list)
grep("https://www.dn.no/dagligvare/handel/coop-vil-selge-103-butikker/1-1-5308961", url.list)
url.list <- url.list[-c(500, 1015, 1016, 4709, 4728, 4819, 5290, 5551, 5655, 5812, 6029, 6042, 6077, 7125, 10823, 15118, 15802, 16111)]
date.list <- date.list[-c(500, 1015, 1016, 4709, 4728, 4819, 5290, 5551, 5655, 5812, 6029, 6042, 6077, 7125, 10823, 15118, 15802, 16111)]
# Log in to DN subscription
# Only run after having closed R/cleaned environment!
url <- "https://www.dn.no/auth/login"
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36"
session <- session(url, user_agent(uastring))
form <- html_form(session)[[1]]
fill <- html_form_set(form,
username = "livewt@live.no",
password = "masterthesis123")
session_submit(session, fill, submit = NULL, config(referer = session$url))
# Extract text from each article
text <- list()
for (url in url.list) {
jump <- session %>%
session_jump_to(url)  # Jump to each URL logged in
html <- read_html(jump) %>%
html_nodes("article") %>%
html_nodes("section") %>%
html_nodes("p")
text <- rbind(text, toString(html))
}
# save(text, file = "text.RData")
# load("text.RData")
# Remove HTML code and everything but letters (not completely finished)
text <- text %>%
str_remove_all("class.*?\\n") %>%
str_remove_all("<span.*?p>") %>%
str_remove_all("<a.*?>") %>%
str_remove_all("class=\"carousel__item-txt carousel--jobbsearch-narrow__item-txt") %>%
str_remove_all("<aside.*?<\\aside") %>%
str_replace_all("<p", " ") %>%
str_replace_all("</p>", " ") %>%
str_replace_all("\n", " ") %>%
str_replace_all("[^[[:alpha:]][[:space:]]]", " ")
# Make a data frame with dates, URLs and text from each article
text = as.data.frame(text)
text$date = as.Date(date.list, "%d.%m.%Y")
text$url = url.list
names(text)[1] <- "text"
# Remove duplicated text
which(duplicated(text$text))
text <- text[!duplicated(text$text), ]
save(text,file="text.Rdata")
# Libraries
library(rvest)
library(RSelenium)
library(httr)
library(stringr)
library(BatchGetSymbols)
library(lexicon)
library(translateR)
library(stopwords)
library(lubridate)
library(RCurl)
library(dplyr)
# NEWS ARTICLE RETRIEVAL
# Extract URLs and dates for each article
articles <- 1
url.list <- as.character()
date.list <- as.character()
files = sort(list.files(pattern="\\.(html)$"), decreasing = T) # get list of .html files
for (file in files) {
URLs <- read_html(file) %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- read_html(file) %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- append(url.list, URLs)
date.list <- append(date.list, Dates)
}
# Are there duplicate urls?
nrow((distinct(as.data.frame(url.list))))
which(duplicated(url.list) == TRUE)
# Removing wrong/not working URLs)
grep("notis", url.list)
grep("https://www.dn.no/marked/2-1-", url.list)
grep("https://www.dn.no/arbeidsliv/2-1", url.list)
grep("https://www.dn.no/personvern/handel/slar-alarm-om-personvern/1-1-5397744", url.list)
grep("https://www.dn.no/borsbarna/finans/mener-aksjer-larer-ungene-om-frykt-og-gradighet/1-1-5331779", url.list)
grep("https://www.dn.no/dagligvare/handel/coop-vil-selge-103-butikker/1-1-5308961", url.list)
url.list <- url.list[-c(500, 1015, 1016, 4709, 4728, 4819, 5290, 5551, 5655, 5812, 6029, 6042, 6077, 7125, 10823, 15118, 15802, 16111)]
date.list <- date.list[-c(500, 1015, 1016, 4709, 4728, 4819, 5290, 5551, 5655, 5812, 6029, 6042, 6077, 7125, 10823, 15118, 15802, 16111)]
# Log in to DN subscription
# Only run after having closed R/cleaned environment!
url <- "https://www.dn.no/auth/login"
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36"
session <- session(url, user_agent(uastring))
form <- html_form(session)[[1]]
fill <- html_form_set(form,
username = "livewt@live.no",
password = "masterthesis123")
session_submit(session, fill, submit = NULL, config(referer = session$url))
# Extract text from each article
text <- list()
for (url in url.list) {
jump <- session %>%
session_jump_to(url)  # Jump to each URL logged in
html <- read_html(jump) %>%
html_nodes("article") %>%
html_nodes("section") %>%
html_nodes("p")
text <- rbind(text, toString(html))
}
# save(text, file = "text.RData")
# load("text.RData")
# Remove HTML code and everything but letters (not completely finished)
text <- text %>%
str_remove_all("class.*?\\n") %>%
str_remove_all("<span.*?p>") %>%
str_remove_all("<a.*?>") %>%
str_remove_all("class=\"carousel__item-txt carousel--jobbsearch-narrow__item-txt") %>%
str_remove_all("<aside.*?<\\aside") %>%
str_replace_all("<p", " ") %>%
str_replace_all("</p>", " ") %>%
str_replace_all("\n", " ") %>%
str_replace_all("[^[[:alpha:]][[:space:]]]", " ")
# Make a data frame with dates, URLs and text from each article
text = as.data.frame(text)
text$date = as.Date(date.list, "%d.%m.%Y")
text$url = url.list
names(text)[1] <- "text"
# Remove duplicated text
which(duplicated(text$text))
text <- text[!duplicated(text$text), ]
save(text,file="text.Rdata")
setwd("C:/Users/Herdis/Desktop/thesis")
# NEWS ARTICLE RETRIEVAL
# Extract URLs and dates for each article
articles <- 1
url.list <- as.character()
date.list <- as.character()
files = sort(list.files(pattern="\\.(html)$"), decreasing = T) # get list of .html files
for (file in files) {
URLs <- read_html(file) %>% # find URLs for each article
html_nodes("h3") %>%
html_nodes("a") %>%
html_attr("href")
Dates <- read_html(file) %>%
html_nodes("time") %>% # find dates for each article
html_attr("datetime")
url.list <- append(url.list, URLs)
date.list <- append(date.list, Dates)
}
# Are there duplicate urls?
nrow((distinct(as.data.frame(url.list))))
which(duplicated(url.list) == TRUE)
# Removing wrong/not working URLs)
grep("notis", url.list)
grep("https://www.dn.no/marked/2-1-", url.list)
grep("https://www.dn.no/arbeidsliv/2-1", url.list)
grep("https://www.dn.no/personvern/handel/slar-alarm-om-personvern/1-1-5397744", url.list)
grep("https://www.dn.no/borsbarna/finans/mener-aksjer-larer-ungene-om-frykt-og-gradighet/1-1-5331779", url.list)
grep("https://www.dn.no/dagligvare/handel/coop-vil-selge-103-butikker/1-1-5308961", url.list)
url.list <- url.list[-c(500, 1015, 1016, 4709, 4728, 4819, 5290, 5551, 5655, 5812, 6029, 6042, 6077, 7125, 10823, 15118, 15802, 16111)]
date.list <- date.list[-c(500, 1015, 1016, 4709, 4728, 4819, 5290, 5551, 5655, 5812, 6029, 6042, 6077, 7125, 10823, 15118, 15802, 16111)]
# Log in to DN subscription
# Only run after having closed R/cleaned environment!
url <- "https://www.dn.no/auth/login"
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36"
session <- session(url, user_agent(uastring))
form <- html_form(session)[[1]]
fill <- html_form_set(form,
username = "livewt@live.no",
password = "masterthesis123")
session_submit(session, fill, submit = NULL, config(referer = session$url))
# Extract text from each article
text <- list()
for (url in url.list) {
jump <- session %>%
session_jump_to(url)  # Jump to each URL logged in
html <- read_html(jump) %>%
html_nodes("article") %>%
html_nodes("section") %>%
html_nodes("p")
text <- rbind(text, toString(html))
}
# save(text, file = "text.RData")
# load("text.RData")
# Remove HTML code and everything but letters (not completely finished)
text <- text %>%
str_remove_all("class.*?\\n") %>%
str_remove_all("<span.*?p>") %>%
str_remove_all("<a.*?>") %>%
str_remove_all("class=\"carousel__item-txt carousel--jobbsearch-narrow__item-txt") %>%
str_remove_all("<aside.*?<\\aside") %>%
str_replace_all("<p", " ") %>%
str_replace_all("</p>", " ") %>%
str_replace_all("\n", " ") %>%
str_replace_all("[^[[:alpha:]][[:space:]]]", " ")
# Make a data frame with dates, URLs and text from each article
text = as.data.frame(text)
text$date = as.Date(date.list, "%d.%m.%Y")
text$url = url.list
names(text)[1] <- "text"
# Remove duplicated text
which(duplicated(text$text))
text <- text[!duplicated(text$text), ]
save(text,file="text.Rdata")
load("text.Rdata")
load("stocks.Rdata")
# Get the names of the companies we have stock price data for
companies = unique(stocks$Company)
# Create a nice data frame for the results
mycols = c("text","date","url",companies)
df = data.frame(matrix(ncol = length(mycols),nrow = nrow(text)))
colnames(df) = mycols
df$date = text$date
df$text = text$text
df$url = text$url
View(df)
# Test run - article 5 mentions at least one company (DNB)
test = df[5,1:3]
# Test run - article 5 mentions at least one company (DNB)
test = df[5,]
for(i in 1:length(c.test)){
for(j in 1:nrow(test)){
for(k in 4:ncol(test)){
test[j,k] =
str_count(test$text[j],c.test[i])
}
# Test run - article 5 mentions at least one company (DNB)
test = df[5,]
c.test = companies[1:50]
for(i in 1:length(c.test)){
for(j in 1:nrow(test)){
for(k in 4:ncol(test)){
test[j,k] =
str_count(test$text[j],c.test[i])
}
View(test)
str_count(as.character(test$text),"DNB")
str_count(as.character(df$text),"DNB")
mentions = list()
for(i in companies){
for(j in 1:nrow(df)){
mentions = str_count(as.character(df$text[j],companies[i]))
}
for(i in companies){
for(j in 1:nrow(df)){
assign(paste0(companies[i],"mentions",sep="."), str_count(as.character(df$text[j],companies[i])))
}
# 1. Which companies are never mentioned?
new.companies = list()
# 1. Which companies are never mentioned?
new.companies = data.frame()
comp.df = data.frame(text$text,text$date)
head(comp.df)
as.character(comp.df$text.text)
comp.df = data.frame(companies,
"mentioned" = NA)
View(comp.df)
for(i in 1:nrow(comp.df)){
comp.df$mentioned[i] =
str_count(as.character(text$text),comp.df$companies[i])
}
warnings()
View(comp.df)
comp.df = data.frame(companies,
"mentioned" = 0)
sum(comp.df$mentioned)
for(i in 1:nrow(comp.df)){
comp.df$mentioned[i] =
str_count(comp.df$companies[i],as.character(text$text))
}
View(comp.df)
grep("DNB",c("Is DNB in here?"))
comp.df = data.frame(companies,
"mentioned" = 0)
for(i in 1:nrow(comp.df)){
comp.df$mentioned[i] =
length(grep(comp.df$companies,as.character(text$text)))
}
